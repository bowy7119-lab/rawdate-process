import json
import os
import stanza
from tqdm import tqdm

# å…¥å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
INPUT_FILE = "egypt_data_enriched.json"
OUTPUT_FILE = "egypt_data_final.json"

def main():
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {INPUT_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Step 1 ã‚’å®Œäº†ã—ã¦ãã ã•ã„ã€‚")
        return

    # 1. Stanzaã®å¤ä»£ã‚®ãƒªã‚·ã‚¢èªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†åˆæœŸåŒ–
    print("ğŸ“¥ Stanzaã®å¤ä»£ã‚®ãƒªã‚·ã‚¢èªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™...")
    stanza.download('grc') # åˆå›ã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒèµ°ã‚Šã¾ã™
    
    print("âš™ï¸ è§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ä¸­...")
    # lemma: è¾æ›¸å½¢åŒ–, pos: å“è©è§£æ
    nlp = stanza.Pipeline('grc', processors='tokenize,lemma,pos', use_gpu=False)

    # 2. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    print(f"ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™: {INPUT_FILE}")
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)

    print(f"ğŸš€ å…¨{len(data)}ä»¶ã®å½¢æ…‹ç´ è§£æï¼ˆãƒ¬ãƒ³ãƒåŒ–ï¼‰ã‚’é–‹å§‹ã—ã¾ã™...")
    
    # 3. è§£æå‡¦ç†
    processed_count = 0
    for entry in tqdm(data, desc="è§£æä¸­"):
        text = entry.get('text', '')
        if not text:
            entry['lemmas'] = []
            entry['tokens'] = []
            continue
            
        try:
            doc = nlp(text)
            
            # æ¤œç´¢ãƒ»åˆ†æç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–ã—ã¦ä¿å­˜
            # lemmas: æ¤œç´¢ç”¨ã®è¾æ›¸å½¢ãƒªã‚¹ãƒˆ (ä¾‹: ['ÎºÎ±Î¹ÏƒÎ±Ï', 'Î¸ÎµÎ¿Ï‚'...])
            # tokens: è©³ç´°åˆ†æç”¨ (ä¾‹: [{'word': 'Î¸ÎµÎ¿Ï…', 'lemma': 'Î¸ÎµÎ¿Ï‚'}, ...])
            lemma_list = []
            token_details = []
            
            for sentence in doc.sentences:
                for word in sentence.words:
                    if word.lemma:
                        lemma_cleaned = word.lemma.lower()
                        lemma_list.append(lemma_cleaned)
                        token_details.append({
                            "word": word.text,       # å®Ÿéš›ã®èªå½¢ (ä¾‹: Î¸ÎµÎ¿Ï…)
                            "lemma": lemma_cleaned,  # è¾æ›¸å½¢ (ä¾‹: Î¸ÎµÎ¿Ï‚)
                            "pos": word.pos          # å“è© (ä¾‹: NOUN)
                        })
            
            entry['lemmas'] = list(set(lemma_list)) # é‡è¤‡æ’é™¤ã—ã¦æ¤œç´¢é«˜é€ŸåŒ–
            entry['analysis'] = token_details       # å††ã‚°ãƒ©ãƒ•ç”¨ã«è©³ç´°ã‚’ä¿å­˜
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºãƒªã‚¹ãƒˆã‚’å…¥ã‚Œã¦ã‚¹ã‚­ãƒƒãƒ—
            entry['lemmas'] = []
            entry['analysis'] = []
        
        processed_count += 1

    # 4. ä¿å­˜
    print(f"ğŸ’¾ è§£æãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¦ã„ã¾ã™: {OUTPUT_FILE}")
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
        
    print("âœ… å®Œäº†ï¼ ã“ã‚Œã§å®Œç’§ãªèªå½¢æ¤œç´¢ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚")
    print("æ¬¡ã¯ app.py ã‚’æ›´æ–°ã—ã¦ã€ã“ã®æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¾ã›ã¾ã—ã‚‡ã†ã€‚")

if __name__ == "__main__":
    main()