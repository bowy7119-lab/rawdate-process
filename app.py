import streamlit as st
import json
import os
import pandas as pd
import plotly.express as px
import chromadb
from openai import OpenAI
from dotenv import load_dotenv
from collections import defaultdict
import unicodedata
import re

# --- åŸºæœ¬è¨­å®š ---
load_dotenv()
st.set_page_config(page_title="Egyptian Greek Inscription Analyzer", layout="wide")

# ãƒ‘ã‚¹è¨­å®š
CHROMA_PATH = "./chroma_db_store"
DATA_FILE = "egypt_data_final.json" 

def get_openai_api_key():
    try:
        if "OPENAI_API_KEY" in st.secrets:
            return st.secrets["OPENAI_API_KEY"]
    except Exception:
        pass
    return os.getenv("OPENAI_API_KEY", "")

api_key = get_openai_api_key()
if not api_key:
    st.error("OPENAI_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚Streamlit secrets ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

client = OpenAI(api_key=api_key)

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼å›ºå®šå¹… & ãƒãƒ£ãƒƒãƒˆå±¥æ­´ ---
if "history" not in st.session_state:
    st.session_state.history = []
if "conversations" not in st.session_state:
    st.session_state.conversations = []
if "active_conversation" not in st.session_state:
    st.session_state.active_conversation = None
if "analysis_history" not in st.session_state:
    st.session_state.analysis_history = []

with st.sidebar:
    st.subheader("ãƒ¢ãƒ¼ãƒ‰é¸æŠ")
    tab_choice = st.radio(
        "",
        ["ğŸ“Š å¹´ä»£æ¨ç§»", "ğŸ’¬ ç¢‘æ–‡ãƒãƒ£ãƒƒãƒˆ"],
        index=0 if st.session_state.get("active_tab") == "ğŸ“Š å¹´ä»£æ¨ç§»" else 1,
    )
    st.session_state["active_tab"] = tab_choice

    st.divider()
    st.markdown(
        """
        <style>
        section[data-testid="stSidebar"] {
            width: 360px !important;
        }
        section[data-testid="stSidebar"] > div {
            width: 360px !important;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )
    if st.session_state.analysis_history:
        st.subheader("åˆ†æå±¥æ­´")
        for idx, item in enumerate(st.session_state.analysis_history[::-1]):
            title = item.get("title", f"Analysis {idx+1}")
            if st.button(f"ğŸ“Š {title}", key=f"analysis_{idx}"):
                st.session_state["analysis_selected"] = item
                st.session_state["active_tab"] = "ğŸ“Š å¹´ä»£æ¨ç§»"
                st.rerun()
    st.subheader("å±¥æ­´")
    if st.session_state.conversations:
        for idx, conv in enumerate(st.session_state.conversations[::-1]):
            title = conv.get("title", f"Conversation {idx+1}")
            col_a, col_b = st.columns([5, 1])
            with col_a:
                if st.button(f"ğŸ’¬ {title}", key=f"conv_{idx}"):
                    st.session_state.history = conv.get("messages", [])
                    st.session_state.active_conversation = conv.get("id")
                    st.session_state["active_tab"] = "ğŸ’¬ ç¢‘æ–‡ãƒãƒ£ãƒƒãƒˆ"
                    st.rerun()
            with col_b:
                if st.button("ğŸ—‘ï¸", key=f"del_conv_{idx}"):
                    conv_id = conv.get("id")
                    st.session_state.conversations = [
                        c for c in st.session_state.conversations if c.get("id") != conv_id
                    ]
                    if st.session_state.active_conversation == conv_id:
                        st.session_state.active_conversation = None
                        st.session_state.history = []
                    st.rerun()

# --- ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ ---
@st.cache_resource
def get_chroma_db():
    if not os.path.exists(CHROMA_PATH): return None
    return chromadb.PersistentClient(path=CHROMA_PATH).get_collection("inscriptions")

@st.cache_data
def load_json_data():
    if not os.path.exists(DATA_FILE): return []
    with open(DATA_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

# --- ğŸ› ï¸ å…±é€šãƒ˜ãƒ«ãƒ‘ãƒ¼: å¼·åŠ›ãªæ­£è¦åŒ– ---
def normalize_text(text):
    """
    ç¢‘æ–‡æ¤œç´¢ç”¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ã™ã‚‹ã€‚
    ã‚¢ã‚¯ã‚»ãƒ³ãƒˆé™¤å»ã€è¨˜å·é™¤å»ã€ç•°ä½“å­—çµ±ä¸€ã‚’è¡Œã†ã€‚
    """
    if not text: return ""
    text = str(text).lower()
    
    # Unicodeæ­£è¦åŒ– (ã‚¢ã‚¯ã‚»ãƒ³ãƒˆåˆ†é›¢ã—ã¦å‰Šé™¤)
    text = ''.join(c for c in unicodedata.normalize('NFD', text)
                   if unicodedata.category(c) != 'Mn')
    
    # è¨˜å·å‰Šé™¤: [ ] ( ) < > { } .
    text = re.sub(r'[\[\]\(\)<>\{\}\.]', '', text)
    
    # ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚·ã‚°ãƒç­‰ã®çµ±ä¸€
    text = text.replace('Ï‚', 'Ïƒ')
    
    return text.strip()

# --- ğŸ§  ã‚¿ãƒ–1ç”¨ãƒ­ã‚¸ãƒƒã‚¯: å˜èªåˆ†æç”¨ã®æ‹¡å¼µ ---
def get_expanded_search_terms(query):
    """(ã‚¿ãƒ–1ç”¨) å˜èªãƒ¬ãƒ™ãƒ«ã§ã®å¤‰åŒ–å½¢å±•é–‹"""
    system_prompt = """
    You are an expert Ancient Greek Philologist.
    Analyze the user's query and return a JSON object with:
    1. "greek_forms": A list of the lemma AND ALL inflected forms.
       Example: "ÎºÎ±Î¹ÏƒÎ±Ï" -> ["ÎºÎ±Î¹ÏƒÎ±Ï", "ÎºÎ±Î¹ÏƒÎ±ÏÎ¿Ï‚", "ÎºÎ±Î¹ÏƒÎ±ÏÎ¹", "ÎºÎ±Î¹ÏƒÎ±ÏÎ±", "ÎºÎ±Î¹ÏƒÎ±ÏÏ‰Î½"]
       IMPORTANT: Normalize them (no accents).
    2. "english_keywords": English translations.
    """
    try:
        res = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": query}],
            response_format={"type": "json_object"},
            temperature=0.0
        )
        return json.loads(res.choices[0].message.content)
    except:
        return {"greek_forms": [query], "english_keywords": [query]}

# --- ğŸ§  ã‚¿ãƒ–2ç”¨ãƒ­ã‚¸ãƒƒã‚¯: ãƒãƒ£ãƒƒãƒˆç”¨ã®é«˜åº¦ãªæ¤œç´¢æˆ¦ç•¥ ---
def get_chat_search_strategy(user_question):
    """
    (ã‚¿ãƒ–2ç”¨) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•(æ—¥æœ¬èªå¯)ã‹ã‚‰ã€æ­´å²çš„èƒŒæ™¯ã‚’è€ƒæ…®ã—ãŸæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    ä¾‹: "ãƒ—ãƒˆãƒ¬ãƒã‚¤ã‚ªã‚¹1ä¸–" -> {"english": ["Ptolemy I", "Soter", "Berenice"], "greek": ["Î Ï„Î¿Î»ÎµÎ¼Î±á¿–Î¿Ï‚", "Î£Ï‰Ï„Î®Ï", "Î’ÎµÏÎµÎ½Î¯ÎºÎ·"]}
    """
    system_prompt = """
    You are an expert Historian of Ptolemaic and Roman Egypt.
    Analyze the user's question and extract key search terms to find relevant Greek inscriptions.
    
    Task:
    1. Identify key historical figures, deities, or concepts in the question.
    2. Expand them to include:
       - Specific epithets (e.g., "Ptolemy I" -> "Soter", "Lagi").
       - Associated family members (e.g., "Berenice").
       - Key Greek terms (e.g., "Basileus", "Synodos").
    3. Return a JSON object with:
       - "english": List of English keywords.
       - "greek": List of Ancient Greek keywords (lemmas or common forms).
    """
    try:
        res = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_question}],
            response_format={"type": "json_object"},
            temperature=0.3 # å°‘ã—å‰µé€ æ€§ã‚’æŒãŸã›ã¦é€£æƒ³ã•ã›ã‚‹
        )
        return json.loads(res.choices[0].message.content)
    except:
        return {"english": [user_question], "greek": []}

# --- ğŸ“Š ã‚¿ãƒ–1ãƒ­ã‚¸ãƒƒã‚¯: ãƒ‡ãƒ¼ã‚¿åˆ†æ (å‰å›ã¨åŒã˜) ---
def analyze_data_robust(data, query):
    years_map = defaultdict(float)
    form_counts = defaultdict(int)
    matched_items = []
    
    expanded = get_expanded_search_terms(query)
    target_greek = set([normalize_text(w) for w in expanded.get('greek_forms', [])])
    target_eng = set([w.lower() for w in expanded.get('english_keywords', [])])
    target_greek.add(normalize_text(query))

    for d in data:
        is_hit = False
        text_raw = d.get('text', '')
        # è¨˜å·ã‚’é™¤å»ã—ã¤ã¤å˜èªåˆ†å‰²
        words_raw = re.split(r'\s+', text_raw)
        
        for w_raw in words_raw:
            w_norm = normalize_text(w_raw)
            if w_norm in target_greek:
                is_hit = True
                if len(w_norm) > 1:
                    form_counts[w_norm] += 1
        
        if not is_hit:
            content_eng = str(d.get('english_translation', '')).lower()
            for eng_key in target_eng:
                if eng_key in content_eng:
                    is_hit = True
                    break

        if is_hit:
            s, e = int(d.get('date_min', 0)), int(d.get('date_max', 0))
            if s == 0 and e == 0: pass 
            else:
                duration = e - s + 1
                weight = 1.0 / duration if duration > 0 else 1.0
                for y in range(s, e + 1):
                    years_map[y] += weight
                matched_items.append(d)
            
    df_trend = pd.DataFrame(list(years_map.items()), columns=["Year", "Frequency"]).sort_values("Year")
    df_pie = pd.DataFrame(list(form_counts.items()), columns=["Form", "Count"]).sort_values("Count", ascending=False)
    
    return df_trend, df_pie, matched_items, list(target_greek)

# --- UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ: å‡ºå…¸ãƒªã‚¹ãƒˆ ---
def render_citation_list(inscriptions, max_items=20, title_prefix="ãƒ’ãƒƒãƒˆã—ãŸç¢‘æ–‡"):
    st.markdown(f"### ğŸ“œ {title_prefix} (Top {min(len(inscriptions), max_items)})")
    
    seen_ids = set()
    unique_items = []
    for item in inscriptions:
        if item['id'] not in seen_ids:
            unique_items.append(item)
            seen_ids.add(item['id'])
            
    for item in unique_items[:max_items]:
        label = f"**ID: {item['id']}** | {item.get('date_min')}~{item.get('date_max')} | {item.get('region_sub', 'Unknown')}"
        with st.expander(label):
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**Original Greek:**")
                st.markdown(f"<div style='word-wrap: break-word; font-family: sans-serif; color: #aaa;'>{item['text']}</div>", unsafe_allow_html=True)
            with col2:
                st.markdown("**English Translation:**")
                st.write(item.get('english_translation', '(No translation)'))

# --- ãƒ¡ã‚¤ãƒ³ UI ---
col_logo, col_title = st.columns([1, 12])
with col_logo:
    st.image("EGIAlogo.png", width=120)
with col_title:
    st.title("Egyptian Greek Inscription Analyzer")
st.caption("Powered by AI & Robust Normalization")

collection = get_chroma_db()
full_data = load_json_data()

if collection is None or not full_data:
    st.error("ãƒ‡ãƒ¼ã‚¿æº–å‚™ãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“ã€‚Step 1/1.5, Step 2 ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    st.stop()

if "active_tab" not in st.session_state:
    st.session_state["active_tab"] = "ğŸ“Š å¹´ä»£æ¨ç§»"

# === Tab 1: å¹´ä»£æ¨ç§» (å®Œæˆæ¸ˆ) ===
if tab_choice == "ğŸ“Š å¹´ä»£æ¨ç§»":
    st.subheader("AIæ¨è«–ã¨æ­£è¦åŒ–ã«ã‚ˆã‚‹å¹´ä»£æ¨ç§»")
    query = st.text_input("æ¤œç´¢èªï¼ˆä¾‹: ÎºÎ±Î¹ÏƒÎ±Ï, ptolemyï¼‰", "ÎºÎ±Î¹ÏƒÎ±Ï")
    
    if st.button("åˆ†æå®Ÿè¡Œ"):
        if not query:
            st.warning("æ¤œç´¢èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        else:
            with st.spinner("AIãŒå¤‰åŒ–å½¢ã‚’å±•é–‹ã—ã€å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ç…§åˆä¸­..."):
                df_trend, df_pie, hits, search_stems = analyze_data_robust(full_data, query)
                
                st.info(f"ğŸ” æ¤œç´¢ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ(æ­£è¦åŒ–æ¸ˆ): {', '.join(list(search_stems)[:15])} ...")
                
                if not df_trend.empty:
                    col1, col2 = st.columns([2, 1])
                    with col1:
                        st.markdown(f"#### ğŸ“ˆ å¹´ä»£æ¨ç§» (Hit: {len(hits)})")
                        fig_line = px.line(df_trend, x="Year", y="Frequency", title=f"Trend: {query}")
                        st.plotly_chart(fig_line, use_container_width=True)
                    with col2:
                        st.markdown("#### ğŸ° èªå½¢å‡ºç¾æ¯”ç‡ (æ­£è¦åŒ–å¾Œ)")
                        if not df_pie.empty:
                            fig_pie = px.pie(df_pie, values="Count", names="Form", title=f"Variations of '{query}'")
                            st.plotly_chart(fig_pie, use_container_width=True)
                        else:
                            st.caption("â€» ã‚®ãƒªã‚·ã‚¢èªå½¢ã®ç›´æ¥ä¸€è‡´ãªã—ï¼ˆè‹±èªæ¦‚å¿µãƒ’ãƒƒãƒˆã®ã¿ï¼‰")
                            
                    render_citation_list(hits, title_prefix="æ¤œç´¢ãƒ’ãƒƒãƒˆ")
                    # Save analysis to sidebar history
                    title = f"{query} ({len(hits)} hits)"
                    st.session_state.analysis_history.append(
                        {
                            "title": title,
                            "query": query,
                            "hits": hits,
                            "trend": df_trend,
                            "pie": df_pie,
                            "search_stems": search_stems,
                        }
                    )
                else:
                    st.warning("è©²å½“ãƒ‡ãƒ¼ã‚¿ãªã—")

    # Show selected analysis from sidebar
    if st.session_state.get("analysis_selected"):
        sel = st.session_state["analysis_selected"]
        st.info(f"ğŸ” éå»ã®åˆ†æ: {sel.get('title')}")
        if sel.get("trend") is not None and not sel["trend"].empty:
            col1, col2 = st.columns([2, 1])
            with col1:
                st.markdown(f"#### ğŸ“ˆ å¹´ä»£æ¨ç§» (Hit: {len(sel.get('hits', []))})")
                fig_line = px.line(sel["trend"], x="Year", y="Frequency", title=f"Trend: {sel.get('query')}")
                st.plotly_chart(fig_line, use_container_width=True)
            with col2:
                st.markdown("#### ğŸ° èªå½¢å‡ºç¾æ¯”ç‡ (æ­£è¦åŒ–å¾Œ)")
                if sel.get("pie") is not None and not sel["pie"].empty:
                    fig_pie = px.pie(sel["pie"], values="Count", names="Form", title=f"Variations of '{sel.get('query')}'")
                    st.plotly_chart(fig_pie, use_container_width=True)
                else:
                    st.caption("â€» ã‚®ãƒªã‚·ã‚¢èªå½¢ã®ç›´æ¥ä¸€è‡´ãªã—ï¼ˆè‹±èªæ¦‚å¿µãƒ’ãƒƒãƒˆã®ã¿ï¼‰")
            render_citation_list(sel.get("hits", []), title_prefix="æ¤œç´¢ãƒ’ãƒƒãƒˆ")

# === Tab 2: ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ (ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆç‰ˆ) ===
if tab_choice == "ğŸ’¬ ç¢‘æ–‡ãƒãƒ£ãƒƒãƒˆ":
    st.subheader("ç¢‘æ–‡ãƒãƒ£ãƒƒãƒˆ")
    if st.button("ğŸ†• æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆ"):
        st.session_state.history = []
        st.session_state.active_conversation = None
    st.markdown("#### AI Model")
    chat_model = st.selectbox(
        "Select Model",
        ["gpt-4o", "gpt-4o-mini"],
        index=0,
        help="gpt-4o: é«˜ç²¾åº¦\ngpt-4o-mini: é«˜é€Ÿ"
    )

    st.markdown(
        """
        <style>
        /* Chat input fixed at bottom */
        div[data-testid="stChatInput"] {
            position: fixed;
            bottom: 0;
            left: 360px;
            right: 0;
            z-index: 1000;
            padding: 1rem 1rem 1.25rem;
            width: calc(100% - 360px);
            background: linear-gradient(180deg, rgba(14,14,18,0) 0%, rgba(14,14,18,0.85) 35%, rgba(14,14,18,1) 100%);
        }
        /* Shift input to align with main content width */
        @media (max-width: 1200px) {
            div[data-testid="stChatInput"] {
                left: 0;
                width: 100%;
            }
        }
        /* Make the input area larger, Gemini-like */
        div[data-testid="stChatInput"] textarea {
            min-height: 72px;
            font-size: 1rem;
            line-height: 1.4;
        }
        div[data-testid="stChatInput"] input {
            min-height: 56px;
            font-size: 1rem;
        }
        /* Keep input aligned with main content (avoid sidebar overlap) */
        div[data-testid="stChatInput"] > div {
            width: 100%;
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 24px;
        }
        /* Align user messages to the right (Gemini-like) */
        div[data-testid="stChatMessage"][data-testid="stChatMessage-user"] {
            display: flex;
            justify-content: flex-end;
        }
        div[data-testid="stChatMessage"][data-testid="stChatMessage-user"] > div {
            display: flex;
            justify-content: flex-end;
        }
        div[data-testid="stChatMessage"][data-testid="stChatMessage-user"] [data-testid="stMarkdownContainer"] {
            display: inline-block;
            text-align: left;
            margin-left: auto;
            background: rgba(255,255,255,0.06);
            border: 1px solid rgba(255,255,255,0.08);
            border-radius: 18px;
            padding: 0.6rem 0.9rem;
            max-width: 75%;
        }
        /* Assistant messages remain left-aligned */
        div[data-testid="stChatMessage"][data-testid="stChatMessage-assistant"] [data-testid="stMarkdownContainer"] {
            max-width: 80%;
        }
        /* Keep content above fixed input */
        .block-container {
            padding-bottom: 8.5rem;
        }
        /* Prevent unintended italics in model output */
        div[data-testid="stChatMessage"] em {
            font-style: normal !important;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )
    
    if "history" not in st.session_state: st.session_state.history = []
    
    for m in st.session_state.history:
        st.chat_message(m["role"]).write(m["content"])
        if m.get("role") == "assistant" and m.get("refs"):
            with st.expander("ğŸ” å‚ç…§ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹"):
                render_citation_list(m["refs"], title_prefix="å‚ç…§ãƒ‡ãƒ¼ã‚¿")
    
    if p := st.chat_input("è³ªå•ã‚’å…¥åŠ›"):
        st.session_state.history.append({"role": "user", "content": p})
        st.chat_message("user").write(p)
        
        with st.spinner(f"{chat_model} ãŒé–¢é€£ç”¨èªã‚’æ¨è«–ã—ã€æ¤œç´¢ä¸­..."):
            
            # 1. æ¤œç´¢æˆ¦ç•¥ã®ç«‹æ¡ˆ (ã“ã“ãŒé€²åŒ–)
            # æ—¥æœ¬èªã®è³ªå•ã‹ã‚‰ã€æ¤œç´¢ã™ã¹ãè‹±èªãƒ»ã‚®ãƒªã‚·ã‚¢èªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
            strategy = get_chat_search_strategy(p)
            
            # æ¤œç´¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ: è‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ + ã‚®ãƒªã‚·ã‚¢èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’çµåˆ
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¯ã€Œæ¦‚å¿µã€ã‚’æ¢ã™ã®ã§ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç¾…åˆ—ã™ã‚‹ã®ãŒåŠ¹æœçš„
            search_query = " ".join(strategy.get('english', []) + strategy.get('greek', []))
            
            # 2. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ (æ¤œç´¢ç¯„å›²ã‚’åºƒã‚ã«30ä»¶)
            q_vec = client.embeddings.create(input=[search_query], model="text-embedding-3-small").data[0].embedding
            results = collection.query(query_embeddings=[q_vec], n_results=30)
            
            # 3. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
            context_str = ""
            ref_data = []
            seen_refs = set()
            id_map = {str(d['id']): d for d in full_data}
            
            for doc, meta in zip(results['documents'][0], results['metadatas'][0]):
                mid = str(meta['id'])
                if mid not in seen_refs:
                    orig = id_map.get(mid)
                    date_min = orig.get("date_min") if orig else ""
                    date_max = orig.get("date_max") if orig else ""
                    region = orig.get("region_sub") if orig else ""
                    context_str += f"[ID: {mid}] Date: {date_min}â€“{date_max}; Region: {region}\n{doc[:600]}...\n\n"
                    if orig:
                        ref_data.append(orig)
                    seen_refs.add(mid)
            
            # 4. å›ç­”ç”Ÿæˆ
            sys_msg = """
            ã‚ãªãŸã¯å¤ä»£ã‚¨ã‚¸ãƒ—ãƒˆãƒ»ã‚®ãƒªã‚·ã‚¢ç¢‘æ–‡ã®å°‚é–€å®¶ã§ã™ã€‚
            æä¾›ã•ã‚ŒãŸã€Contextã€‘(è‹±è¨³ä»˜ãç¢‘æ–‡)ã‚’æ ¹æ‹ ã¨ã—ã¦ç”¨ã„ã€
            ä¸è¶³ã™ã‚‹æ­´å²çš„èƒŒæ™¯ã¯ä¸€èˆ¬çŸ¥è­˜ã¨ã—ã¦è£œã„ãªãŒã‚‰ã€è³ªå•ã«å¯¾ã—ã¦æ—¥æœ¬èªã§è©³ç´°ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
            
            ãƒ«ãƒ¼ãƒ«:
            1. ç¢‘æ–‡ã‹ã‚‰æ ¹æ‹ ã‚’å¼•ãå ´åˆã¯ã€å¿…ãš [ID: xxxxx] ã®å½¢å¼ã§å‡ºå…¸ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚
            1. å¯èƒ½ãªé™ã‚Šå¤šãã®è©²å½“ç¢‘æ–‡ã‚’å¼•ç”¨ã—ã€ä»£è¡¨çš„ãªã‚‚ã®ã¯è¤‡æ•°æŒ™ã’ã¦ãã ã•ã„ï¼ˆå°‘ãªãã¨ã‚‚6ä»¶ä»¥ä¸Šã‚’ç›®æ¨™ï¼‰ã€‚
            2. å¼•ç”¨ã™ã‚‹ç¢‘æ–‡ã«ã¤ã„ã¦ã¯å¿…ãšå¹´ä»£ï¼ˆdate_minã€œdate_maxï¼‰ã‚’æ˜ç¤ºã—ã€ãã®å¹´ä»£èƒŒæ™¯ã‚’åŠ å‘³ã—ã¦è§£èª¬ã—ã¦ãã ã•ã„ã€‚
            3. å„ç¢‘æ–‡ã®è§£èª¬ã«ã¯ã€Œå¹´ä»£èƒŒæ™¯ãƒ»åœ°åŸŸãƒ»äº‹è±¡ï¼ˆå®—æ•™/æ”¿æ²»/ç¤¾ä¼šï¼‰ã€ã®ã„ãšã‚Œã‹ã‚’å«ã‚ã€å¹´ä»£ã«å³ã—ãŸåˆ†æã‚’å¿…ãšè¡Œã£ã¦ãã ã•ã„ã€‚
            4. æ­´å²çš„èƒŒæ™¯ãƒ»ä¸€èˆ¬çŸ¥è­˜ã§è£œè¶³ã™ã‚‹éƒ¨åˆ†ã¯æ–‡é ­ã«ã€ŒèƒŒæ™¯çŸ¥è­˜:ã€ã¨æ˜ç¤ºã—ã€å‡ºå…¸IDã¯ä»˜ã‘ãªã„ã§ãã ã•ã„ã€‚
            5. æ–‡è„ˆã‹ã‚‰ã€ç¢‘æ–‡ã®è¨˜è¿°ãŒè³ªå•ã«é–¢é€£ã™ã‚‹ç†ç”±ã‚’è£œè¶³ã—ã¦ãã ã•ã„ã€‚
            6. ç¢‘æ–‡ä¸­ã®è¨˜å·ï¼ˆ[ ]ãªã©ï¼‰ã¯ã€èª­ã¿ã‚„ã™ã„ã‚ˆã†ã«è£œã£ã¦è§£é‡ˆã—ã¦ãã ã•ã„ã€‚
            7. å›ç­”ã¯ååˆ†ã«é•·ãã€è©³ç´°ã«ã—ã¦ãã ã•ã„ã€‚
            """
            
            try:
                ans_res = client.chat.completions.create(
                    model=chat_model,
                    messages=[
                        {"role": "system", "content": sys_msg},
                        {"role": "user", "content": f"Context:\n{context_str}\n\nQuestion: {p}"}
                    ]
                )
                ans = (ans_res.choices[0].message.content or "").strip()
            except Exception as e:
                ans = f"å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚\n\nè©³ç´°: {e}"
            if not ans:
                ans = "å›ç­”ãŒç©ºã§ã—ãŸã€‚å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
            
        st.chat_message("assistant").write(ans)
        st.session_state.history.append(
            {"role": "assistant", "content": ans, "refs": ref_data}
        )

        # Save conversation summary into sidebar list
        summary_title = p[:24] + ("â€¦" if len(p) > 24 else "")
        if st.session_state.active_conversation is None:
            conv_id = f"conv_{len(st.session_state.conversations)+1}"
            st.session_state.conversations.append(
                {
                    "id": conv_id,
                    "title": summary_title,
                    "messages": st.session_state.history.copy(),
                    "refs": ref_data,
                }
            )
            st.session_state.active_conversation = conv_id
        else:
            for conv in st.session_state.conversations:
                if conv.get("id") == st.session_state.active_conversation:
                    conv["messages"] = st.session_state.history.copy()
                    conv["refs"] = ref_data
                    if conv.get("title", "") == "":
                        conv["title"] = summary_title
                    break
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã€Œã©ã‚“ãªè¨€è‘‰ã§æ¤œç´¢ã—ãŸã‹ã€ã‚’è¦‹ã›ã‚‹ï¼ˆé€æ˜æ€§ï¼‰
        with st.expander("ğŸ” AIã®æ¤œç´¢æˆ¦ç•¥ & å‚ç…§ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹"):
            st.info(f"**AIãŒç”Ÿæˆã—ãŸæ¤œç´¢èª:**\n- English: {', '.join(strategy.get('english', []))}\n- Greek: {', '.join(strategy.get('greek', []))}")
            render_citation_list(ref_data, title_prefix="å‚ç…§ãƒ‡ãƒ¼ã‚¿")
