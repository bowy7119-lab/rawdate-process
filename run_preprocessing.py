import json
import os
import time
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ãƒ¢ãƒ‡ãƒ«ã¯æœ€å¼·ã® gpt-4o ã‚’ä½¿ç”¨
MODEL = "gpt-4o"
INPUT_FILE = 'egypt_processed_tagged.json'

def analyze_long_inscription(entry):
    """è¶…é•·æ–‡å°‚ç”¨ã®è§£æãƒ­ã‚¸ãƒƒã‚¯"""
    text = entry['text']
    metadata = entry['metadata']
    
    # 15,000æ–‡å­—ã§ã‚«ãƒƒãƒˆï¼ˆåˆ†æã«ã¯ååˆ†ï¼‰ã—ã€å‡ºåŠ›ã‚’ã€Œé‡è¦èª30å€‹ã€ã«å³æ ¼ã«åˆ¶é™
    prompt = f"""
    Analyze this massive Ancient Greek inscription (Decree).
    Metadata: {metadata}
    Text: {text[:15000]}

    Instructions:
    1. Extract exactly 5 conceptual English keywords.
    2. Extract exactly 30 of the most HISTORICALLY SIGNIFICANT Greek lemmas (Kings, Gods, Places, specific terms). 
    3. DO NOT output a long list. Keep the JSON response compact to avoid truncation.

    Output format:
    {{"keywords": ["...", "..."], "lemmas": ["...", "..." ]}}
    """

    try:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": "You are a specialist in Epigraphy. Output valid JSON only."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0,
            max_tokens=2000 # å‡ºåŠ›æ ã‚’ååˆ†ã«ç¢ºä¿
        )
        result = json.loads(response.choices[0].message.content)
        entry['lemmas'] = result.get('lemmas', [])
        entry['keywords'] = result.get('keywords', [])
        print(f"âœ… ID {entry['id']} rescued successfully!")
        return entry
    except Exception as e:
        print(f"âŒ ID {entry['id']} failed again: {e}")
        # æœ€çµ‚æ‰‹æ®µï¼šã‚¿ã‚°ã ã‘æ‰‹å‹•é¢¨ã«ä»˜ã‘ã¦é€šã™
        entry['lemmas'] = []
        entry['keywords'] = ["Major Decree", "Long Text", "Ptolemaic"]
        return entry

def main():
    if not os.path.exists(INPUT_FILE):
        print("ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)

    updated_count = 0
    for entry in data:
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ãƒ¬ãƒãŒç©ºï¼ˆï¼éå»ã«å¤±æ•—ã—ãŸãƒ‡ãƒ¼ã‚¿ï¼‰ã ã‘ã‚’å‡¦ç†
        if not entry.get('keywords') or len(entry.get('keywords', [])) == 0:
            print(f"ğŸ” Rescuing ID {entry['id']} (Length: {len(entry['text'])})...")
            analyze_long_inscription(entry)
            updated_count += 1
            
            # 1ä»¶ã”ã¨ã«ä¿å­˜ï¼ˆç¢ºå®Ÿæ€§ã‚’æœŸã™ãŸã‚ï¼‰
            with open(INPUT_FILE, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            
            time.sleep(1) # APIåˆ¶é™å›é¿

    print(f"ğŸ‰ ãƒ¬ã‚¹ã‚­ãƒ¥ãƒ¼å®Œäº†ï¼ {updated_count} ä»¶ã®ç¢‘æ–‡ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()